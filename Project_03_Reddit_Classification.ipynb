{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Reddit is a popular online discussion board where people can create groups, commonly known as 'subreddits', to chat and share content with like-minded people around the world. \n",
    "\n",
    "Today, we will be looking at how we can use machine learning to make an attempt in trying to predict sentiments, more specifically sarcasm/humour/irony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Natural Language Processing (NLP) has been one of the longest running computing science fields, starting all the way back from the 1950s with Alan Turing's infamous Turing test - where a machine is said to pass the test if an evaluator cannot tell, behind the scenes, who is the one truly answering a posed question: Machine? or human?\n",
    "\n",
    "Whilst it sounds like a rather trivial scenario, many decades on we've still yet to truly crack the language code, something that befuddles even the best of us mere humans. That is not to say we've made little progress - we've come far enough to now have chatbots being able to understand, guide, and assist us in services such as product support centres, as well as the more advanced voice assistants residing in each and every smartphone being made today.\n",
    "\n",
    "Language, however, is not something static. It's not just a mere tool for simple communication. Language is a dynamic, fluid entity that can be moulded to resemble something, but yet truly mean something else. Things like humour, irony, and sarcasm can be complex and at times, thought-intensive to decipher, let alone create.\n",
    "\n",
    "This project will use tools such as Scikit-Learn, Keras and the various feature selection tools out there to find out from Reddit, what differentiates a legitimate science question (r/askscience) from a humourous/sarcastic pseudoscience question (r/shittyaskscience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries for this project\n",
    "\n",
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.python.keras import backend as k\n",
    "from xgboost import XGBClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PRAW as Reddit's API wrapper to start scraping.\n",
    "\n",
    "r = praw.Reddit(client_id='nNc6VxDLF9gUtw',\n",
    "                client_secret='mmtCGSil2ML5ncye6axK_gAO8wg',\n",
    "                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.1 Safari/605.1.15')\n",
    "\n",
    "sas = r.subreddit('shittyaskscience')\n",
    "sci = r.subreddit('askscience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating dataframe and getting all posts limited to 10k for subreddit. Scrape begins.\n",
    "\n",
    "sas_df = pd.DataFrame((), columns=['id', 'title', 'author', 'score'])\n",
    "sas_new = sas.new(limit=10000)\n",
    "sas_list = [(post.id, post.title, post.author, post.score) for post in sas_new]\n",
    "sas_df_list = pd.DataFrame(sas_list, columns=['id', 'title', 'author', 'score'])\n",
    "sas_df = pd.concat([sas_df, sas_df_list], axis=0)\n",
    "sas_df.reset_index()\n",
    "sas_df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "\n",
    "sas_df.to_csv('./datasets/sas.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_df = pd.read_csv('./datasets/sas.csv') # Loading csv file back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chpnge</td>\n",
       "      <td>If the two European heatwaves this year were c...</td>\n",
       "      <td>Asian_Canadaball</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cho8v8</td>\n",
       "      <td>Who invented being gay?</td>\n",
       "      <td>TigerpanzerIV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chmghz</td>\n",
       "      <td>Why did gravity ignore this bird?</td>\n",
       "      <td>zlicht</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chm4p1</td>\n",
       "      <td>Do cats hunt watermelons?</td>\n",
       "      <td>killerbunnyfamily</td>\n",
       "      <td>856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chkqxs</td>\n",
       "      <td>If air is such a good insulator, why so I stil...</td>\n",
       "      <td>hudzell</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  chpnge  If the two European heatwaves this year were c...   \n",
       "1  cho8v8                            Who invented being gay?   \n",
       "2  chmghz                  Why did gravity ignore this bird?   \n",
       "3  chm4p1                          Do cats hunt watermelons?   \n",
       "4  chkqxs  If air is such a good insulator, why so I stil...   \n",
       "\n",
       "              author  score  \n",
       "0   Asian_Canadaball      1  \n",
       "1      TigerpanzerIV      2  \n",
       "2             zlicht      0  \n",
       "3  killerbunnyfamily    856  \n",
       "4            hudzell      6  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_df.head() # Head looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(996, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_df.shape # Checking shape to see what we've got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating dataframe and getting all posts limited to 10k for subreddit\n",
    "\n",
    "sci_df = pd.DataFrame((), columns=['id', 'title', 'author', 'score'])\n",
    "sci_new = sci.new(limit=10000)\n",
    "sci_list = [(post.id, post.title, post.author, post.score) for post in sci_new]\n",
    "sci_df_list = pd.DataFrame(sci_list, columns=['id', 'title', 'author', 'score'])\n",
    "sci_df = pd.concat([sci_df, sci_df_list], axis=0)\n",
    "sci_df.reset_index()\n",
    "sci_df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "\n",
    "sci_df.to_csv('./datasets/sci.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_df = pd.read_csv('./datasets/sci.csv') # Loading csv file back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sci_df.shape # Checking shape to see what we've got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chm70g</td>\n",
       "      <td>AskScience AMA Series: We're from the Pacific ...</td>\n",
       "      <td>AskScienceModerator</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chkmci</td>\n",
       "      <td>Why are beta blockers restricted to prescripti...</td>\n",
       "      <td>dontknowhowtoprogram</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chiol7</td>\n",
       "      <td>To my best understanding, space is (for the mo...</td>\n",
       "      <td>ctcsoccer17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chij26</td>\n",
       "      <td>Is Ceres a dwarf planet or an asteroid?</td>\n",
       "      <td>louisprimaasamonkey</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chh4nk</td>\n",
       "      <td>If the event horizon is the region in space wh...</td>\n",
       "      <td>bigmaxporter</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  chm70g  AskScience AMA Series: We're from the Pacific ...   \n",
       "1  chkmci  Why are beta blockers restricted to prescripti...   \n",
       "2  chiol7  To my best understanding, space is (for the mo...   \n",
       "3  chij26            Is Ceres a dwarf planet or an asteroid?   \n",
       "4  chh4nk  If the event horizon is the region in space wh...   \n",
       "\n",
       "                 author  score  \n",
       "0   AskScienceModerator    350  \n",
       "1  dontknowhowtoprogram      5  \n",
       "2           ctcsoccer17      3  \n",
       "3   louisprimaasamonkey      3  \n",
       "4          bigmaxporter      3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sci_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning subreddit encoding to each dataframe\n",
    "\n",
    "sci_df['type'] = 'sci'\n",
    "sas_df['type'] = 'sas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([sas_df, sci_df], axis=0) # Joining both subreddits' data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1746, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id', 'score', 'author'], axis=1, inplace=True) # Dropping unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'] = df['type'].replace({'sas':1, 'sci':0}) # Creating binary elements for subreddit type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    False\n",
       "type     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any() # Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep=False, inplace=True) # Dropping any duplicate posts with identical titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title'] # Setting up features and target columns\n",
    "y = df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up into training and testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=34, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right here we are, we've finally settled down with our training and test data sets.\n",
    "\n",
    "So let's start things off with a simple CountVectorizer from scikit to tokenise words.\n",
    "\n",
    "There'll be a few iterations for this, with the parameters being:\n",
    "\n",
    "- lowercase (True/False): \n",
    "There might be a difference between having non-lowercase words and lowercase words \n",
    "between the 2 subs, this will be something that I'll switch and see if it improves.   \n",
    "\n",
    "- stop_words (english/none): \n",
    "Having stop_words on 'english' might seem like a no-brainer, but we aren't dealing with\n",
    "proper text here. These are all one-phrase post titles where intended \"stylistic\"/wrong phrasing of words\n",
    "might make a difference between the 2 subs.   \n",
    "\n",
    "- n-grams (default/(1,2)/(1,3)): \n",
    "Having different number of words grouped as features might make make a difference\n",
    "since I'm expecting humour/sarcastic science questions to have more common question terms (since they don't need\n",
    "to get to specifics eg \"Why is the protein that binds VCAM-1 called Very Late Antigen-4 (VLA-4)?\" vs \n",
    "\"what kind of bird is this?\"))\n",
    "\n",
    "- min_df/max_df (default/0.25-0.5):\n",
    "Somewhat counterintuitive if I'm switching stop_words to False, but this will also be tweaked around, mostly to see if there might be a certain proportion that hits an acceptable spot.\n",
    "\n",
    "- max_features:\n",
    "To be adjusted during model tweaking to get the right range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(lowercase=True, stop_words=None, ngram_range=(1,2), min_df=3, max_df=0.3)\n",
    "\n",
    "X_train_cvec  = pd.DataFrame(cvec.fit_transform(X_train).todense(), columns=cvec.get_feature_names())\n",
    "X_test_cvec = pd.DataFrame(cvec.transform(X_test).todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219, 1351)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.757144302772718\n",
      "Test Score: 0.7667304015296367\n",
      "\n",
      "LR CVEC Matrix:\n",
      "True Negatives: 154\n",
      "False Positives: 52\n",
      "False Negatives: 70\n",
      "True Positives: 247\n",
      "Accuracy: 0.7667304015296367\n",
      "AUC ROC: 0.7633763131297663\n"
     ]
    }
   ],
   "source": [
    "# Running a simple Logistic Regression as a baseline\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "lr_model = lr.fit(X_train_cvec, y_train)\n",
    "lr_model_score = cross_val_score(lr_model, X_train_cvec, y_train, cv=5)\n",
    "\n",
    "print(\"CV Score: \" + str(lr_model_score.mean()))\n",
    "print(\"Test Score: \" + str(lr_model.score(X_test_cvec, y_test)) + \"\\n\")\n",
    "\n",
    "lr_pred = pd.DataFrame(lr_model.predict(X_test_cvec))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, lr_pred).ravel()\n",
    "lr_cvec_auc = roc_auc_score(y_test, lr_pred)\n",
    "lr_cvec_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "lr_cvec_df = pd.DataFrame([[tn,fp,fn,tp,lr_cvec_acc,lr_cvec_auc]], index=['LR_CVEC'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"LR CVEC Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(lr_cvec_acc))\n",
    "print(\"AUC ROC: {}\".format(lr_cvec_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7563448694596235\n",
      "Test Score: 0.7954110898661568\n",
      "\n",
      "MultinomialNB CVEC Matrix:\n",
      "True Negatives: 160\n",
      "False Positives: 46\n",
      "False Negatives: 61\n",
      "True Positives: 256\n",
      "Accuracy: 0.7954110898661568\n",
      "AUC ROC: 0.7921350035220972\n"
     ]
    }
   ],
   "source": [
    "# Running a basic Multinomial Naive Bayes as a baseline\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.5)\n",
    "\n",
    "mnb_model = mnb.fit(X_train_cvec, y_train)\n",
    "mnb_model_score = cross_val_score(mnb_model, X_train_cvec, y_train, cv=5)\n",
    "\n",
    "print(\"CV Score: \" + str(mnb_model_score.mean()))\n",
    "print(\"Test Score: \" + str(mnb_model.score(X_test_cvec, y_test)) + \"\\n\")\n",
    "\n",
    "mnb_pred = mnb_model.predict(X_test_cvec)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, mnb_pred).ravel()\n",
    "mnb_cvec_auc = roc_auc_score(y_test, mnb_pred)\n",
    "mnb_cvec_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "mnb_cvec_df = pd.DataFrame([[tn,fp,fn,tp,mnb_cvec_acc,mnb_cvec_auc]], index=['MNB_CVEC'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"MultinomialNB CVEC Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(mnb_cvec_acc))\n",
    "print(\"AUC ROC: {}\".format(mnb_cvec_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7612696485191932\n",
      "Test Score: 0.7858508604206501\n",
      "\n",
      "BernoulliNB CVEC Matrix:\n",
      "True Negatives: 153\n",
      "False Positives: 53\n",
      "False Negatives: 59\n",
      "True Positives: 258\n",
      "Accuracy: 0.7858508604206501\n",
      "AUC ROC: 0.7782992863924536\n"
     ]
    }
   ],
   "source": [
    "# Running a basic Bernoulli Naive Bayes as a baseline\n",
    "\n",
    "bn = BernoulliNB(alpha=0.4)\n",
    "\n",
    "bn_model = bn.fit(X_train_cvec, y_train)\n",
    "bn_model_score = cross_val_score(bn_model, X_train_cvec, y_train, cv=5)\n",
    "\n",
    "print(\"CV Score: \" + str(bn_model_score.mean()))\n",
    "print(\"Test Score: \" + str(bn_model.score(X_test_cvec, y_test)) + \"\\n\")\n",
    "\n",
    "bn_pred = bn_model.predict(X_test_cvec)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, bn_pred).ravel()\n",
    "bn_cvec_auc = roc_auc_score(y_test, bn_pred)\n",
    "bn_cvec_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "bn_cvec_df = pd.DataFrame([[tn,fp,fn,tp,bn_cvec_acc,bn_cvec_auc]], index=['BN_CVEC'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"BernoulliNB CVEC Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(bn_cvec_acc))\n",
    "print(\"AUC ROC: {}\".format(bn_cvec_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ACC</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNB_CVEC</th>\n",
       "      <td>160</td>\n",
       "      <td>46</td>\n",
       "      <td>61</td>\n",
       "      <td>256</td>\n",
       "      <td>0.795411</td>\n",
       "      <td>0.792135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_CVEC</th>\n",
       "      <td>153</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>258</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.778299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_CVEC</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>247</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.763376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           TN  FP  FN   TP       ACC       AUC\n",
       "MNB_CVEC  160  46  61  256  0.795411  0.792135\n",
       "BN_CVEC   153  53  59  258  0.785851  0.778299\n",
       "LR_CVEC   154  52  70  247  0.766730  0.763376"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pd.concat([lr_cvec_df, mnb_cvec_df, bn_cvec_df], axis=0)\n",
    "score.sort_values('AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Run\n",
    "\n",
    "We now have the scores from our first batch run of 3 models. As expected, the Naive Bayes are doing better than the regular Logistic Regression model.   \n",
    "\n",
    "It must be noted that due to the nature of the similarities in content, tracking the choice of model via its pure accuracy can be flawed, as we can be getting a lot of hits on True Positives due to our imbalanced data whilst clocking up a sizeable number of False Positives. I've gone with tracking the ranking via the AUC instead.\n",
    "\n",
    "We'll use the TF-IDF vectoriser next for our run of our models once more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF vectoriser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=None, ngram_range=(1,2), max_df=0.3)\n",
    "X_train_tfidf  = pd.DataFrame(tfidf.fit_transform(X_train).todense(), columns=tfidf.get_feature_names())\n",
    "X_test_tfidf = pd.DataFrame(tfidf.transform(X_test).todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7243859584894352\n",
      "Test Score: 0.7973231357552581\n",
      "\n",
      "LR TF-IDF Matrix:\n",
      "True Negatives: 138\n",
      "False Positives: 68\n",
      "False Negatives: 38\n",
      "True Positives: 279\n",
      "Accuracy: 0.7973231357552581\n",
      "AUC ROC: 0.7750145477933293\n"
     ]
    }
   ],
   "source": [
    "# Running logistic regression with TFIDF\n",
    "\n",
    "lr_model1 = lr.fit(X_train_tfidf, y_train)\n",
    "lr_model1_score = cross_val_score(lr_model1, X_train_tfidf, y_train, cv=7)\n",
    "\n",
    "print(\"CV Score: \" + str(lr_model1_score.mean()))\n",
    "print(\"Test Score: \" + str(lr_model1.score(X_test_tfidf, y_test)) + \"\\n\")\n",
    "\n",
    "lr1_pred = lr_model1.predict(X_test_tfidf)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, lr1_pred).ravel()\n",
    "lr_tfidf_auc = roc_auc_score(y_test, lr1_pred)\n",
    "lr_tfidf_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "lr_tfidf_df = pd.DataFrame([[tn,fp,fn,tp,lr_tfidf_acc,lr_tfidf_auc]], index=['LR_TFIDF'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"LR TF-IDF Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(lr_tfidf_acc))\n",
    "print(\"AUC ROC: {}\".format(lr_tfidf_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7613467136904593\n",
      "Test Score: 0.8087954110898662\n",
      "\n",
      "MultinomialNB TF-IDF Matrix:\n",
      "True Negatives: 131\n",
      "False Positives: 75\n",
      "False Negatives: 25\n",
      "True Positives: 292\n",
      "Accuracy: 0.8087954110898662\n",
      "AUC ROC: 0.7785289883923923\n"
     ]
    }
   ],
   "source": [
    "# Running Multinomial NB with TFIDF\n",
    "\n",
    "mnb1 = MultinomialNB(alpha=0.4)\n",
    "\n",
    "mnb_model1 = mnb1.fit(X_train_tfidf, y_train)\n",
    "mnb_model1_score = cross_val_score(mnb_model1, X_train_tfidf, y_train, cv=7)\n",
    "\n",
    "print(\"CV Score: \" + str(mnb_model1_score.mean()))\n",
    "print(\"Test Score: \" + str(mnb_model1.score(X_test_tfidf, y_test)) + \"\\n\")\n",
    "\n",
    "mnb1_pred = mnb_model1.predict(X_test_tfidf)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, mnb1_pred).ravel()\n",
    "mnb_tfidf_auc = roc_auc_score(y_test, mnb1_pred)\n",
    "mnb_tfidf_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "mnb_tfidf_df = pd.DataFrame([[tn,fp,fn,tp,mnb_tfidf_acc,mnb_tfidf_auc]], index=['MNB_TFIDF'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"MultinomialNB TF-IDF Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(mnb_tfidf_acc))\n",
    "print(\"AUC ROC: {}\".format(mnb_tfidf_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7425712577237392\n",
      "Test Score: 0.7992351816443595\n",
      "\n",
      "BernoulliNB CVEC Matrix:\n",
      "True Negatives: 123\n",
      "False Positives: 83\n",
      "False Negatives: 22\n",
      "True Positives: 295\n",
      "Accuracy: 0.7992351816443595\n",
      "AUC ROC: 0.7638433738629751\n"
     ]
    }
   ],
   "source": [
    "# Running Bernoulli NB with TFIDF\n",
    "\n",
    "bn1 = BernoulliNB(alpha=0.2)\n",
    "\n",
    "bn_model1 = bn1.fit(X_train_tfidf, y_train)\n",
    "bn_model1_score = cross_val_score(bn_model1, X_train_tfidf, y_train,cv=7)\n",
    "\n",
    "print(\"CV Score: \" + str(bn_model1_score.mean()))\n",
    "print(\"Test Score: \" + str(bn_model1.score(X_test_tfidf, y_test)) + \"\\n\")\n",
    "\n",
    "bn1_pred = bn_model1.predict(X_test_tfidf)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, bn1_pred).ravel()\n",
    "bn_tfidf_auc = roc_auc_score(y_test, bn1_pred)\n",
    "bn_tfidf_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "bn_tfidf_df = pd.DataFrame([[tn,fp,fn,tp,bn_tfidf_acc,bn_tfidf_auc]], index=['BN_TFIDF'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"BernoulliNB CVEC Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(bn_tfidf_acc))\n",
    "print(\"AUC ROC: {}\".format(bn_tfidf_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ACC</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNB_CVEC</th>\n",
       "      <td>160</td>\n",
       "      <td>46</td>\n",
       "      <td>61</td>\n",
       "      <td>256</td>\n",
       "      <td>0.795411</td>\n",
       "      <td>0.792135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB_TFIDF</th>\n",
       "      <td>131</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "      <td>292</td>\n",
       "      <td>0.808795</td>\n",
       "      <td>0.778529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_CVEC</th>\n",
       "      <td>153</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>258</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.778299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_TFIDF</th>\n",
       "      <td>138</td>\n",
       "      <td>68</td>\n",
       "      <td>38</td>\n",
       "      <td>279</td>\n",
       "      <td>0.797323</td>\n",
       "      <td>0.775015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_TFIDF</th>\n",
       "      <td>123</td>\n",
       "      <td>83</td>\n",
       "      <td>22</td>\n",
       "      <td>295</td>\n",
       "      <td>0.799235</td>\n",
       "      <td>0.763843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_CVEC</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>247</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.763376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TN  FP  FN   TP       ACC       AUC\n",
       "MNB_CVEC   160  46  61  256  0.795411  0.792135\n",
       "MNB_TFIDF  131  75  25  292  0.808795  0.778529\n",
       "BN_CVEC    153  53  59  258  0.785851  0.778299\n",
       "LR_TFIDF   138  68  38  279  0.797323  0.775015\n",
       "BN_TFIDF   123  83  22  295  0.799235  0.763843\n",
       "LR_CVEC    154  52  70  247  0.766730  0.763376"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pd.concat([lr_cvec_df,mnb_cvec_df,bn_cvec_df,lr_tfidf_df,mnb_tfidf_df,bn_tfidf_df])\n",
    "score.sort_values('AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Run\n",
    "\n",
    "Checking out the results from the 2nd run, it's clear that the Multinomial NB model is our top model choice, for both the CVEC and TFIDF runs. The other 2 models seem to be middling at the moment, we'll proceed with feature engineering our data with lemmatisation, as well as doing hyperparameter tweakings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceeding with Lemmatisation with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # Loading SpaCy and its large English core kit for lemmatisation.\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def lem_sentences(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lem'] = df['title'].apply(lem_sentences) # Applying our function to lemmatise the post titles\n",
    "X_lem = df['lem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our training and testing data\n",
    "\n",
    "X_lem_train, X_lem_test, y_train, y_test = train_test_split(X_lem, y, test_size=0.3, random_state=34, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising our count vectoriser with our lemmatised data\n",
    "\n",
    "cvec = CountVectorizer(lowercase=True, stop_words=None, ngram_range=(1,2), min_df=3, max_df=0.5)\n",
    "\n",
    "X_lem_train_cvec  = pd.DataFrame(cvec.fit_transform(X_lem_train).todense(), columns=cvec.get_feature_names())\n",
    "X_lem_test_cvec = pd.DataFrame(cvec.transform(X_lem_test).todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219, 1358)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lem_train_cvec.shape # Checking shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7449213354865566\n",
      "Test Score: 0.7667304015296367\n",
      "\n",
      "LR2 CVEC Matrix:\n",
      "True Negatives: 154\n",
      "False Positives: 52\n",
      "False Negatives: 70\n",
      "True Positives: 247\n",
      "Accuracy: 0.7667304015296367\n",
      "AUC ROC: 0.7633763131297663\n"
     ]
    }
   ],
   "source": [
    "# Running logistic regression on lemmatised data\n",
    "\n",
    "lr2_model = lr.fit(X_lem_train_cvec, y_train)\n",
    "lr2_model_score = cross_val_score(lr2_model, X_lem_train_cvec, y_train, cv=7)\n",
    "\n",
    "print(\"CV Score: \" + str(lr2_model_score.mean()))\n",
    "print(\"Test Score: \" + str(lr2_model.score(X_lem_test_cvec, y_test)) + \"\\n\")\n",
    "\n",
    "lr2_pred = pd.DataFrame(lr2_model.predict(X_lem_test_cvec))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, lr2_pred).ravel()\n",
    "lr2_cvec_auc = roc_auc_score(y_test, lr2_pred)\n",
    "lr2_cvec_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "lr2_cvec_df = pd.DataFrame([[tn,fp,fn,tp,lr2_cvec_acc,lr2_cvec_auc]], index=['LR2_CVEC'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"LR2 CVEC Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(lr2_cvec_acc))\n",
    "print(\"AUC ROC: {}\".format(lr2_cvec_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7727787897186804\n",
      "Test Score: 0.7782026768642447\n",
      "\n",
      "MultinomialNB2 CVEC Matrix:\n",
      "True Negatives: 159\n",
      "False Positives: 47\n",
      "False Negatives: 69\n",
      "True Positives: 248\n",
      "Accuracy: 0.7782026768642447\n",
      "AUC ROC: 0.7770895225261094\n"
     ]
    }
   ],
   "source": [
    "# Running Multinomial NB on lemmatised data\n",
    "\n",
    "mnb2 = MultinomialNB(alpha=0.5)\n",
    "\n",
    "mnb_model2 = mnb2.fit(X_lem_train_cvec, y_train)\n",
    "mnb_model2_score = cross_val_score(mnb_model2, X_lem_train_cvec, y_train, cv=5)\n",
    "\n",
    "print(\"CV Score: \"+ str(mnb_model2_score.mean()))\n",
    "print(\"Test Score: \"+ str(mnb_model2.score(X_lem_test_cvec, y_test)) + \"\\n\")\n",
    "\n",
    "mnb2_pred = mnb_model2.predict(X_lem_test_cvec)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, mnb2_pred).ravel()\n",
    "mnb2_cvec_auc = roc_auc_score(y_test, mnb2_pred)\n",
    "mnb2_cvec_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "mnb2_cvec_df = pd.DataFrame([[tn,fp,fn,tp,mnb2_cvec_acc,mnb2_cvec_auc]], index=['MNB2_CVEC'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"MultinomialNB2 CVEC Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(mnb2_cvec_acc))\n",
    "print(\"AUC ROC: {}\".format(mnb2_cvec_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7719422519058221\n",
      "Test Score: 0.7820267686424475\n",
      "\n",
      "BernoulliNB CVEC Matrix:\n",
      "True Negatives: 157\n",
      "False Positives: 49\n",
      "False Negatives: 65\n",
      "True Positives: 252\n",
      "Accuracy: 0.7820267686424475\n",
      "AUC ROC: 0.7785443018590549\n"
     ]
    }
   ],
   "source": [
    "# Running Bernoulli NB on lemmatised data\n",
    "\n",
    "bn2 = BernoulliNB(alpha=0.2)\n",
    "\n",
    "bn_model2 = bn2.fit(X_lem_train_cvec, y_train)\n",
    "bn_model2_score = cross_val_score(bn_model2, X_lem_train_cvec, y_train, cv=5)\n",
    "\n",
    "print(\"CV Score: \"+ str(bn_model2_score.mean()))\n",
    "print(\"Test Score: \" + str(bn_model2.score(X_lem_test_cvec, y_test)) + \"\\n\")\n",
    "\n",
    "bn2_pred = bn_model2.predict(X_lem_test_cvec)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, bn2_pred).ravel()\n",
    "bn2_cvec_auc = roc_auc_score(y_test, bn2_pred)\n",
    "bn2_cvec_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "bn2_cvec_df = pd.DataFrame([[tn,fp,fn,tp,bn2_cvec_acc,bn2_cvec_auc]], index=['BN2_CVEC'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"BernoulliNB CVEC Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(bn2_cvec_acc))\n",
    "print(\"AUC ROC: {}\".format(bn2_cvec_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ACC</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BN2_CVEC</th>\n",
       "      <td>157</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>252</td>\n",
       "      <td>0.782027</td>\n",
       "      <td>0.778544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB2_CVEC</th>\n",
       "      <td>159</td>\n",
       "      <td>47</td>\n",
       "      <td>69</td>\n",
       "      <td>248</td>\n",
       "      <td>0.778203</td>\n",
       "      <td>0.777090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR2_CVEC</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>247</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.763376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TN  FP  FN   TP       ACC       AUC\n",
       "BN2_CVEC   157  49  65  252  0.782027  0.778544\n",
       "MNB2_CVEC  159  47  69  248  0.778203  0.777090\n",
       "LR2_CVEC   154  52  70  247  0.766730  0.763376"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2 = pd.concat([lr2_cvec_df, mnb2_cvec_df, bn2_cvec_df], axis=0).sort_values('AUC', ascending=False)\n",
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running TFIDF vectoriser on lemmatised data\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=None, ngram_range=(1,2), max_df=0.3)\n",
    "X_lem_train_tfidf  = pd.DataFrame(tfidf.fit_transform(X_lem_train).todense(), columns=tfidf.get_feature_names())\n",
    "X_lem_test_tfidf = pd.DataFrame(tfidf.transform(X_lem_test).todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219, 13879)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lem_train_tfidf.shape # Checking shape of TFIDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7400234847776064\n",
      "Test Score: 0.780114722753346\n",
      "\n",
      "LR3 TF-IDF Matrix:\n",
      "True Negatives: 132\n",
      "False Positives: 74\n",
      "False Negatives: 41\n",
      "True Positives: 276\n",
      "Accuracy: 0.780114722753346\n",
      "AUC ROC: 0.7557195797984746\n"
     ]
    }
   ],
   "source": [
    "# Running logistic regression on TFIDF lemmatised data\n",
    "\n",
    "lr_model3 = lr.fit(X_lem_train_tfidf, y_train)\n",
    "lr_model3_score = cross_val_score(lr_model3, X_lem_train_tfidf, y_train, cv=7)\n",
    "\n",
    "print(\"CV Score: \" + str(lr_model3_score.mean()))\n",
    "print(\"Test Score: \" + str(lr_model3.score(X_lem_test_tfidf, y_test)) + \"\\n\")\n",
    "\n",
    "lr3_pred = lr_model3.predict(X_lem_test_tfidf)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, lr3_pred).ravel()\n",
    "lr3_tfidf_auc = roc_auc_score(y_test, lr3_pred)\n",
    "lr3_tfidf_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "lr3_tfidf_df = pd.DataFrame([[tn,fp,fn,tp,lr3_tfidf_acc,lr3_tfidf_auc]], index=['LR3_TFIDF'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"LR3 TF-IDF Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(lr3_tfidf_acc))\n",
    "print(\"AUC ROC: {}\".format(lr3_tfidf_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7654848074370271\n",
      "Test Score: 0.7896749521988528\n",
      "\n",
      "MultinomialNB3 TF-IDF Matrix:\n",
      "True Negatives: 130\n",
      "False Positives: 76\n",
      "False Negatives: 34\n",
      "True Positives: 283\n",
      "Accuracy: 0.7896749521988528\n",
      "AUC ROC: 0.7619062203301583\n"
     ]
    }
   ],
   "source": [
    "# Running Multinomail NB on TFIDF lemmatised data\n",
    "\n",
    "mnb3 = MultinomialNB(alpha=0.3)\n",
    "mnb_model3 = mnb3.fit(X_lem_train_tfidf, y_train)\n",
    "mnb_model3_score = cross_val_score(mnb_model3, X_lem_train_tfidf, y_train, cv=7)\n",
    "\n",
    "print(\"CV Score: \" + str(mnb_model3_score.mean()))\n",
    "print(\"Test Score: \" + str(mnb_model3.score(X_lem_test_tfidf, y_test)) + \"\\n\")\n",
    "\n",
    "mnb3_pred = mnb_model3.predict(X_lem_test_tfidf)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, mnb3_pred).ravel()\n",
    "mnb3_tfidf_auc = roc_auc_score(y_test, mnb3_pred)\n",
    "mnb3_tfidf_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "mnb3_tfidf_df = pd.DataFrame([[tn,fp,fn,tp,mnb3_tfidf_acc,mnb3_tfidf_auc]], index=['MNB3_TFIDF'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"MultinomialNB3 TF-IDF Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(mnb3_tfidf_acc))\n",
    "print(\"AUC ROC: {}\".format(mnb3_tfidf_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7606387262897983\n",
      "Test Score: 0.7858508604206501\n",
      "\n",
      "BernoulliNB TF-IDF Matrix:\n",
      "True Negatives: 119\n",
      "False Positives: 87\n",
      "False Negatives: 25\n",
      "True Positives: 292\n",
      "Accuracy: 0.7858508604206501\n",
      "AUC ROC: 0.7494027748001593\n"
     ]
    }
   ],
   "source": [
    "# Running Bernoulli NB on TFIDF lemmatised data\n",
    "\n",
    "bn3 = BernoulliNB(alpha=0.2)\n",
    "\n",
    "bn_model3 = bn3.fit(X_lem_train_tfidf, y_train)\n",
    "bn_model3_score = cross_val_score(bn_model3, X_lem_train_tfidf, y_train, cv=7)\n",
    "\n",
    "\n",
    "print(\"CV Score: \" + str(bn_model3_score.mean()))\n",
    "print(\"Test Score: \" + str(bn_model3.score(X_lem_test_tfidf, y_test)) + \"\\n\")\n",
    "\n",
    "bn3_pred = bn_model3.predict(X_lem_test_tfidf)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, bn3_pred).ravel()\n",
    "bn3_tfidf_auc = roc_auc_score(y_test, bn3_pred)\n",
    "bn3_tfidf_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "bn3_tfidf_df = pd.DataFrame([[tn,fp,fn,tp,bn3_tfidf_acc,bn3_tfidf_auc]], index=['BN3_TFIDF'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"BernoulliNB TF-IDF Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(bn3_tfidf_acc))\n",
    "print(\"AUC ROC: {}\".format(bn3_tfidf_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ACC</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BN2_CVEC</th>\n",
       "      <td>157</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>252</td>\n",
       "      <td>0.782027</td>\n",
       "      <td>0.778544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB2_CVEC</th>\n",
       "      <td>159</td>\n",
       "      <td>47</td>\n",
       "      <td>69</td>\n",
       "      <td>248</td>\n",
       "      <td>0.778203</td>\n",
       "      <td>0.777090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR2_CVEC</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>247</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.763376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB3_TFIDF</th>\n",
       "      <td>130</td>\n",
       "      <td>76</td>\n",
       "      <td>34</td>\n",
       "      <td>283</td>\n",
       "      <td>0.789675</td>\n",
       "      <td>0.761906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR3_TFIDF</th>\n",
       "      <td>132</td>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>276</td>\n",
       "      <td>0.780115</td>\n",
       "      <td>0.755720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN3_TFIDF</th>\n",
       "      <td>119</td>\n",
       "      <td>87</td>\n",
       "      <td>25</td>\n",
       "      <td>292</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.749403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TN  FP  FN   TP       ACC       AUC\n",
       "BN2_CVEC    157  49  65  252  0.782027  0.778544\n",
       "MNB2_CVEC   159  47  69  248  0.778203  0.777090\n",
       "LR2_CVEC    154  52  70  247  0.766730  0.763376\n",
       "MNB3_TFIDF  130  76  34  283  0.789675  0.761906\n",
       "LR3_TFIDF   132  74  41  276  0.780115  0.755720\n",
       "BN3_TFIDF   119  87  25  292  0.785851  0.749403"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score3 = pd.concat([lr2_cvec_df, mnb2_cvec_df, bn2_cvec_df,lr3_tfidf_df,mnb3_tfidf_df,bn3_tfidf_df])\n",
    "score3.sort_values('AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd Run\n",
    "\n",
    "Amongst our model runs with lemmatisation, it looks like the TF-IDF vectorisers didn't do much to help the cause, perhaps it was dropping off too many rare terms that were indeed important to discern between the two subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning via Pipeline, GridSearchCV\n",
    "\n",
    "The next step will be to run a pipeline to find the best parameters to use for each of our models as well as a corresponding suited vectoriser setting using our lemmatised data from before. \n",
    "\n",
    "Here we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3600 candidates, totalling 10800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 774 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1474 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2374 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 3474 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 4774 tasks      | elapsed:   54.4s\n",
      "[Parallel(n_jobs=-1)]: Done 6274 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7974 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9874 tasks      | elapsed:  1.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "\tcvec__lowercase: 'True'\n",
      "\tcvec__max_df: 0.2\n",
      "\tcvec__min_df: 2\n",
      "\tcvec__ngram_range: (1, 2)\n",
      "\tcvec__stop_words: None\n",
      "\tmnb__alpha: 0.2\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 10800 out of 10800 | elapsed:  2.1min finished\n"
     ]
    }
   ],
   "source": [
    "# Setting up of pipeline and the parameters to be iterated through to find best settings for Multinomial NB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('mnb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'cvec__lowercase': ('True', 'False'),\n",
    "    'cvec__stop_words': (None, 'english'),    \n",
    "    'cvec__min_df': (1, 2, 3),\n",
    "    'cvec__max_df': (0.1, 0.2, 0.3, 0.4, 0.5),\n",
    "    'cvec__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'mnb__alpha': (0.1, 0.2, 0.3, 0.4, 0.5)\n",
    "}\n",
    "\n",
    "# Running pipeline to find best settings in 3 folds.\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_lem_train, y_train)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up count vectoriser with settings from pipeline\n",
    "\n",
    "cvec = CountVectorizer(lowercase=True, stop_words=None, ngram_range=(1,2), min_df=2, max_df=0.2)\n",
    "\n",
    "X_lem_train_cvec  = pd.DataFrame(cvec.fit_transform(X_lem_train).todense(), columns=cvec.get_feature_names())\n",
    "X_lem_test_cvec = pd.DataFrame(cvec.transform(X_lem_test).todense(), columns=cvec.get_feature_names())\n",
    "\n",
    "trans = TfidfTransformer(norm='l2', use_idf=False)\n",
    "X_lem_train_final = trans.fit_transform(X_lem_train_cvec)\n",
    "X_lem_test_final = trans.transform(X_lem_test_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.8087954110898662\n",
      "\n",
      "MultinomialNB3 TF-IDF Matrix:\n",
      "True Negatives: 153\n",
      "False Positives: 53\n",
      "False Negatives: 47\n",
      "True Positives: 270\n",
      "Accuracy: 0.8087954110898662\n",
      "AUC ROC: 0.7972267311874063\n"
     ]
    }
   ],
   "source": [
    "# Setting up Multinomial NB with settings from pipeline\n",
    "\n",
    "mnb4 = MultinomialNB(alpha=0.2)\n",
    "mnb_model4 = mnb4.fit(X_lem_train_final, y_train)\n",
    "\n",
    "print(\"Test Score: \" + str(mnb_model4.score(X_lem_test_final, y_test)) + \"\\n\")\n",
    "\n",
    "mnb4_pred = mnb_model4.predict(X_lem_test_final)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, mnb4_pred).ravel()\n",
    "mnb4_final_auc = roc_auc_score(y_test, mnb4_pred)\n",
    "mnb4_final_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "mnb4_final_df = pd.DataFrame([[tn,fp,fn,tp,mnb4_final_acc,mnb4_final_auc]], index=['MNB_FINAL'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"MultinomialNB3 TF-IDF Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(mnb4_final_acc))\n",
    "print(\"AUC ROC: {}\".format(mnb4_final_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3600 candidates, totalling 10800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1632 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 3032 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=-1)]: Done 4832 tasks      | elapsed:   51.1s\n",
      "[Parallel(n_jobs=-1)]: Done 7032 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9632 tasks      | elapsed:  1.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "\tbn__alpha: 0.5\n",
      "\tcvec__lowercase: 'True'\n",
      "\tcvec__max_df: 0.4\n",
      "\tcvec__min_df: 2\n",
      "\tcvec__ngram_range: (1, 3)\n",
      "\tcvec__stop_words: None\n",
      "\ttfidf__norm: 'l1'\n",
      "\ttfidf__use_idf: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 10800 out of 10800 | elapsed:  2.0min finished\n"
     ]
    }
   ],
   "source": [
    "# Setting up of pipeline and the parameters to be iterated through to find best settings for Bernoulli NB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('bn', BernoulliNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'cvec__lowercase': ('True', 'False'),\n",
    "    'cvec__stop_words': (None, 'english'),    \n",
    "    'cvec__min_df': (1, 2, 3),\n",
    "    'cvec__max_df': (0.1, 0.2, 0.3, 0.4, 0.5),\n",
    "    'cvec__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'bn__alpha': (0.1, 0.2, 0.3, 0.4, 0.5)\n",
    "}\n",
    "\n",
    "# Running pipeline to find best settings in 3 folds.\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_lem_train, y_train)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up count vectoriser with settings from pipeline\n",
    "\n",
    "cvec = CountVectorizer(lowercase=True, stop_words=None, ngram_range=(1,3), min_df=2, max_df=0.4)\n",
    "\n",
    "X_lem_train_cvec  = pd.DataFrame(cvec.fit_transform(X_lem_train).todense(), columns=cvec.get_feature_names())\n",
    "X_lem_test_cvec = pd.DataFrame(cvec.transform(X_lem_test).todense(), columns=cvec.get_feature_names())\n",
    "\n",
    "trans = TfidfTransformer(norm='l1', use_idf='True')\n",
    "X_lem_train_final = trans.fit_transform(X_lem_train_cvec)\n",
    "X_lem_test_final = trans.transform(X_lem_test_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.7858508604206501\n",
      "\n",
      "BernoulliNB Final TF-IDF Matrix:\n",
      "True Negatives: 154\n",
      "False Positives: 52\n",
      "False Negatives: 60\n",
      "True Positives: 257\n",
      "Accuracy: 0.7858508604206501\n",
      "AUC ROC: 0.7791491837922268\n"
     ]
    }
   ],
   "source": [
    "# Setting up Bernoulil NB with settings from pipeline\n",
    "\n",
    "bn4 = BernoulliNB(alpha=0.5)\n",
    "\n",
    "bn_model4 = bn4.fit(X_lem_train_final, y_train)\n",
    "\n",
    "print(\"Test Score: \" + str(bn_model4.score(X_lem_test_final, y_test)) + \"\\n\")\n",
    "\n",
    "bn4_pred = bn_model4.predict(X_lem_test_final)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, bn4_pred).ravel()\n",
    "bn4_final_auc = roc_auc_score(y_test, bn4_pred)\n",
    "bn4_final_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "bn4_final_df = pd.DataFrame([[tn,fp,fn,tp,bn4_final_acc,bn4_final_auc]], index=['BN_FINAL'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"BernoulliNB Final TF-IDF Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(bn4_final_acc))\n",
    "print(\"AUC ROC: {}\".format(bn4_final_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ACC</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNB_FINAL</th>\n",
       "      <td>153</td>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>270</td>\n",
       "      <td>0.808795</td>\n",
       "      <td>0.797227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_FINAL</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>60</td>\n",
       "      <td>257</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.779149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TN  FP  FN   TP       ACC       AUC\n",
       "MNB_FINAL  153  53  47  270  0.808795  0.797227\n",
       "BN_FINAL   154  52  60  257  0.785851  0.779149"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_bayes_final = pd.concat([mnb4_final_df, bn4_final_df], axis=0)\n",
    "score_bayes_final.sort_values('AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4th Run\n",
    "\n",
    "There's a good improvement in both models, with the Multinomial NB model now hitting 0.80 accuracy with a close-enough AUC of 0.79."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "The XGBoost classifier can be used here as well, and it'll be used to compare to how the other models we have above has done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.69977164804751\n",
      "Test Score: 0.6959847036328872\n",
      "\n",
      "XGBClassifier Final TF-IDF Matrix:\n",
      "True Negatives: 140\n",
      "False Positives: 66\n",
      "False Negatives: 93\n",
      "True Positives: 224\n",
      "Accuracy: 0.6959847036328872\n",
      "AUC ROC: 0.6931181280818351\n"
     ]
    }
   ],
   "source": [
    "# Setting up XGBoost run using our TFIDF data, this can take up to 2 mins to run\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb = xgb.fit(X_lem_train_tfidf, y_train)\n",
    "xgb_score = cross_val_score(xgb, X_lem_train_tfidf, y_train, cv=3)\n",
    "\n",
    "print(\"CV Score: \" + str(xgb_score.mean()))\n",
    "print(\"Test Score: \" + str(xgb.score(X_lem_test_tfidf, y_test)) + \"\\n\")\n",
    "\n",
    "xgb_pred = xgb.predict(X_lem_test_tfidf)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, xgb_pred).ravel()\n",
    "xgb_final_auc = roc_auc_score(y_test, xgb_pred)\n",
    "xgb_final_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "xgb_final_df = pd.DataFrame([[tn,fp,fn,tp,xgb_final_acc,xgb_final_auc]], index=['XGB_TFIDF'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"XGBClassifier Final TF-IDF Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(xgb_final_acc))\n",
    "print(\"AUC ROC: {}\".format(xgb_final_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "\txgb__alpha: 0\n",
      "\txgb__eta: 0.1\n",
      "\txgb__gamma: 2\n",
      "\txgb__max_depth: 4\n"
     ]
    }
   ],
   "source": [
    "# Setting up of pipeline and the parameters to be iterated through to find best settings for XGBoost, using our\n",
    "# lemmatised data this time.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('xgb', XGBClassifier()),\n",
    "])\n",
    "\n",
    "parameters = {    \n",
    "    'xgb__eta': (0.1, 0.2),\n",
    "    'xgb__gamma': (0, 1, 2, 3, 4),\n",
    "    'xgb__max_depth': (2, 3, 4),\n",
    "    'xgb__alpha': (0, 0.1, 0.2, 0.3)\n",
    "}\n",
    "\n",
    "# Running pipeline to find best settings in 3 folds\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_lem_train_final, y_train)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7120828844966777\n",
      "Test Score: 0.7036328871892925\n",
      "\n",
      "XGBClassifier Final Matrix:\n",
      "True Negatives: 140\n",
      "False Positives: 66\n",
      "False Negatives: 89\n",
      "True Positives: 228\n",
      "Accuracy: 0.7036328871892925\n",
      "AUC ROC: 0.6994272763468193\n"
     ]
    }
   ],
   "source": [
    "# Running XGBoost with tuned parameters\n",
    "\n",
    "xgb1 = XGBClassifier(eta=0.1, alpha=0, gamma=2, max_depth=4)\n",
    "\n",
    "xgb1 = xgb1.fit(X_lem_train_final, y_train)\n",
    "xgb1_score = cross_val_score(xgb1, X_lem_train_final, y_train, cv=3)\n",
    "\n",
    "print(\"CV Score: \" + str(xgb1_score.mean()))\n",
    "print(\"Test Score: \" + str(xgb1.score(X_lem_test_final, y_test)) + \"\\n\")\n",
    "\n",
    "xgb1_pred = xgb1.predict(X_lem_test_final)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, xgb1_pred).ravel()\n",
    "xgb1_final_auc = roc_auc_score(y_test, xgb1_pred)\n",
    "xgb1_final_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "xgb1_final_df = pd.DataFrame([[tn,fp,fn,tp,xgb1_final_acc,xgb1_final_auc]], index=['XGB_FINAL'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"XGBClassifier Final Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(xgb1_final_acc))\n",
    "print(\"AUC ROC: {}\".format(xgb1_final_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ACC</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNB_FINAL</th>\n",
       "      <td>153</td>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>270</td>\n",
       "      <td>0.808795</td>\n",
       "      <td>0.797227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB_CVEC</th>\n",
       "      <td>160</td>\n",
       "      <td>46</td>\n",
       "      <td>61</td>\n",
       "      <td>256</td>\n",
       "      <td>0.795411</td>\n",
       "      <td>0.792135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_FINAL</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>60</td>\n",
       "      <td>257</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.779149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN2_CVEC</th>\n",
       "      <td>157</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>252</td>\n",
       "      <td>0.782027</td>\n",
       "      <td>0.778544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB_TFIDF</th>\n",
       "      <td>131</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "      <td>292</td>\n",
       "      <td>0.808795</td>\n",
       "      <td>0.778529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_CVEC</th>\n",
       "      <td>153</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>258</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.778299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB2_CVEC</th>\n",
       "      <td>159</td>\n",
       "      <td>47</td>\n",
       "      <td>69</td>\n",
       "      <td>248</td>\n",
       "      <td>0.778203</td>\n",
       "      <td>0.777090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_TFIDF</th>\n",
       "      <td>138</td>\n",
       "      <td>68</td>\n",
       "      <td>38</td>\n",
       "      <td>279</td>\n",
       "      <td>0.797323</td>\n",
       "      <td>0.775015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_TFIDF</th>\n",
       "      <td>123</td>\n",
       "      <td>83</td>\n",
       "      <td>22</td>\n",
       "      <td>295</td>\n",
       "      <td>0.799235</td>\n",
       "      <td>0.763843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR2_CVEC</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>247</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.763376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_CVEC</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>247</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.763376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB3_TFIDF</th>\n",
       "      <td>130</td>\n",
       "      <td>76</td>\n",
       "      <td>34</td>\n",
       "      <td>283</td>\n",
       "      <td>0.789675</td>\n",
       "      <td>0.761906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR3_TFIDF</th>\n",
       "      <td>132</td>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>276</td>\n",
       "      <td>0.780115</td>\n",
       "      <td>0.755720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN3_TFIDF</th>\n",
       "      <td>119</td>\n",
       "      <td>87</td>\n",
       "      <td>25</td>\n",
       "      <td>292</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.749403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_TFIDF</th>\n",
       "      <td>140</td>\n",
       "      <td>66</td>\n",
       "      <td>93</td>\n",
       "      <td>224</td>\n",
       "      <td>0.695985</td>\n",
       "      <td>0.693118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TN  FP  FN   TP       ACC       AUC\n",
       "MNB_FINAL   153  53  47  270  0.808795  0.797227\n",
       "MNB_CVEC    160  46  61  256  0.795411  0.792135\n",
       "BN_FINAL    154  52  60  257  0.785851  0.779149\n",
       "BN2_CVEC    157  49  65  252  0.782027  0.778544\n",
       "MNB_TFIDF   131  75  25  292  0.808795  0.778529\n",
       "BN_CVEC     153  53  59  258  0.785851  0.778299\n",
       "MNB2_CVEC   159  47  69  248  0.778203  0.777090\n",
       "LR_TFIDF    138  68  38  279  0.797323  0.775015\n",
       "BN_TFIDF    123  83  22  295  0.799235  0.763843\n",
       "LR2_CVEC    154  52  70  247  0.766730  0.763376\n",
       "LR_CVEC     154  52  70  247  0.766730  0.763376\n",
       "MNB3_TFIDF  130  76  34  283  0.789675  0.761906\n",
       "LR3_TFIDF   132  74  41  276  0.780115  0.755720\n",
       "BN3_TFIDF   119  87  25  292  0.785851  0.749403\n",
       "XGB_TFIDF   140  66  93  224  0.695985  0.693118"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores = pd.concat([xgb_final_df, score_bayes_final, score3, score])\n",
    "final_scores.sort_values('AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Run\n",
    "\n",
    "Our quick run on XGBoost didn't turn out the results we hoped for, and that can be down to not having enough parameters for it to be tweaked due to computational limits. The TF-IDF data would've taken too long as well for hyperparameter tuning due to the number of folds required to have a good gauge of which settings to use.\n",
    "\n",
    "Next, we'll also include in a simple LSTM model from Keras via Tensorflow to compare against the models we already have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LSTM model run (Tensorflow 2.0 beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title'].values\n",
    "y = df['type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Running the same procedures before to tokenise our text data, but using Keras' processing instead\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokeniser_obj = Tokenizer(lower=True)\n",
    "total_posts = X\n",
    "tokeniser_obj.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4923"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Typically, max_length should be the maximum length of the longest post title, but due to computing constraints\n",
    "# a length of 20 is used, which is mostly fine as our post titles don't usually exceed 20 words.\n",
    "\n",
    "max_length = 20 \n",
    "vocab_size = len(tokeniser_obj.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our training and testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=34, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our tokenised words into sequences, and then padding them for equal length to be fed into our model\n",
    "\n",
    "X_train_tokens = tokeniser_obj.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokeniser_obj.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 20, 200)           984600    \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,305,601\n",
      "Trainable params: 1,305,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The model will be running a single LSTM layer with 200 units in it.\n",
    "\"\"\"\n",
    "\n",
    "embedding_dim = 200 # Word dimension matrix of 200 used\n",
    "batch_size = 24 # Model sees 24 of our training samples at a time\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1219 samples, validate on 523 samples\n",
      "Epoch 1/3\n",
      "1219/1219 - 3s - loss: 0.6876 - accuracy: 0.5570 - val_loss: 0.6504 - val_accuracy: 0.7151\n",
      "Epoch 2/3\n",
      "1219/1219 - 1s - loss: 0.3967 - accuracy: 0.8376 - val_loss: 0.4776 - val_accuracy: 0.7782\n",
      "Epoch 3/3\n",
      "1219/1219 - 1s - loss: 0.1336 - accuracy: 0.9606 - val_loss: 0.4981 - val_accuracy: 0.7897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ff0cad30>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train, epochs=3, batch_size=batch_size, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_pred = model.predict(X_test_pad) # Getting predictions and converting them into binary from their probabilities\n",
    "\n",
    "tf_df = pd.DataFrame(tf_pred, columns=['y_hat'])\n",
    "tf_df['y_hat'] = tf_df['y_hat'].apply(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Final Matrix:\n",
      "True Negatives: 154\n",
      "False Positives: 52\n",
      "False Negatives: 58\n",
      "True Positives: 259\n",
      "Accuracy: 0.7896749521988528\n",
      "AUC ROC: 0.7823037579247191\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, tf_df).ravel()\n",
    "tf_final_auc = roc_auc_score(y_test, tf_df)\n",
    "tf_final_acc = (tn+tp)/(tn+fp+fn+tp)\n",
    "tf_final_df = pd.DataFrame([[tn,fp,fn,tp,tf_final_acc,tf_final_auc]], index=['LSTM_FINAL'], columns=['TN','FP','FN','TP','ACC','AUC'])\n",
    "\n",
    "print(\"LSTM Final Matrix:\")\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "print(\"Accuracy: {}\".format(tf_final_acc))\n",
    "print(\"AUC ROC: {}\".format(tf_final_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ACC</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNB_FINAL</th>\n",
       "      <td>153</td>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>270</td>\n",
       "      <td>0.808795</td>\n",
       "      <td>0.797227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB_CVEC</th>\n",
       "      <td>160</td>\n",
       "      <td>46</td>\n",
       "      <td>61</td>\n",
       "      <td>256</td>\n",
       "      <td>0.795411</td>\n",
       "      <td>0.792135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM_FINAL</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>58</td>\n",
       "      <td>259</td>\n",
       "      <td>0.789675</td>\n",
       "      <td>0.782304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_FINAL</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>60</td>\n",
       "      <td>257</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.779149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN2_CVEC</th>\n",
       "      <td>157</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>252</td>\n",
       "      <td>0.782027</td>\n",
       "      <td>0.778544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB_TFIDF</th>\n",
       "      <td>131</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "      <td>292</td>\n",
       "      <td>0.808795</td>\n",
       "      <td>0.778529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_CVEC</th>\n",
       "      <td>153</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>258</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.778299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB2_CVEC</th>\n",
       "      <td>159</td>\n",
       "      <td>47</td>\n",
       "      <td>69</td>\n",
       "      <td>248</td>\n",
       "      <td>0.778203</td>\n",
       "      <td>0.777090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_TFIDF</th>\n",
       "      <td>138</td>\n",
       "      <td>68</td>\n",
       "      <td>38</td>\n",
       "      <td>279</td>\n",
       "      <td>0.797323</td>\n",
       "      <td>0.775015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_TFIDF</th>\n",
       "      <td>123</td>\n",
       "      <td>83</td>\n",
       "      <td>22</td>\n",
       "      <td>295</td>\n",
       "      <td>0.799235</td>\n",
       "      <td>0.763843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR2_CVEC</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>247</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.763376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_CVEC</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>247</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.763376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB3_TFIDF</th>\n",
       "      <td>130</td>\n",
       "      <td>76</td>\n",
       "      <td>34</td>\n",
       "      <td>283</td>\n",
       "      <td>0.789675</td>\n",
       "      <td>0.761906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR3_TFIDF</th>\n",
       "      <td>132</td>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>276</td>\n",
       "      <td>0.780115</td>\n",
       "      <td>0.755720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN3_TFIDF</th>\n",
       "      <td>119</td>\n",
       "      <td>87</td>\n",
       "      <td>25</td>\n",
       "      <td>292</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.749403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_TFIDF</th>\n",
       "      <td>140</td>\n",
       "      <td>66</td>\n",
       "      <td>93</td>\n",
       "      <td>224</td>\n",
       "      <td>0.695985</td>\n",
       "      <td>0.693118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TN  FP  FN   TP       ACC       AUC\n",
       "MNB_FINAL   153  53  47  270  0.808795  0.797227\n",
       "MNB_CVEC    160  46  61  256  0.795411  0.792135\n",
       "LSTM_FINAL  154  52  58  259  0.789675  0.782304\n",
       "BN_FINAL    154  52  60  257  0.785851  0.779149\n",
       "BN2_CVEC    157  49  65  252  0.782027  0.778544\n",
       "MNB_TFIDF   131  75  25  292  0.808795  0.778529\n",
       "BN_CVEC     153  53  59  258  0.785851  0.778299\n",
       "MNB2_CVEC   159  47  69  248  0.778203  0.777090\n",
       "LR_TFIDF    138  68  38  279  0.797323  0.775015\n",
       "BN_TFIDF    123  83  22  295  0.799235  0.763843\n",
       "LR2_CVEC    154  52  70  247  0.766730  0.763376\n",
       "LR_CVEC     154  52  70  247  0.766730  0.763376\n",
       "MNB3_TFIDF  130  76  34  283  0.789675  0.761906\n",
       "LR3_TFIDF   132  74  41  276  0.780115  0.755720\n",
       "BN3_TFIDF   119  87  25  292  0.785851  0.749403\n",
       "XGB_TFIDF   140  66  93  224  0.695985  0.693118"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scorers = pd.concat([final_scores, tf_final_df])\n",
    "final_scorers.sort_values('AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Run\n",
    "\n",
    "##### AUC ranking (above):   \n",
    "\n",
    "After running all our models with various settings and hyperparameter tweaks, it's clear that our hyperparameter-tuned Multinomial NB model did the best, with the cvec version of it coming in second. Keras' LSTM model rounds it off at 3rd with a single LSTM layer.\n",
    "\n",
    "##### ACC ranking (below):   \n",
    "\n",
    "Similar story on the accuracy ranking, with the Multinomial NB models taking top 2, albeit the TF-IDF version in 2nd place instead of the CVEC version. Looks like the TF-IDF versions of the models did well in the accuracy section, with non-TFIDF versions taking up the other spots below the top 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>ACC</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNB_FINAL</th>\n",
       "      <td>153</td>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>270</td>\n",
       "      <td>0.808795</td>\n",
       "      <td>0.797227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB_TFIDF</th>\n",
       "      <td>131</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "      <td>292</td>\n",
       "      <td>0.808795</td>\n",
       "      <td>0.778529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_TFIDF</th>\n",
       "      <td>123</td>\n",
       "      <td>83</td>\n",
       "      <td>22</td>\n",
       "      <td>295</td>\n",
       "      <td>0.799235</td>\n",
       "      <td>0.763843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_TFIDF</th>\n",
       "      <td>138</td>\n",
       "      <td>68</td>\n",
       "      <td>38</td>\n",
       "      <td>279</td>\n",
       "      <td>0.797323</td>\n",
       "      <td>0.775015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB_CVEC</th>\n",
       "      <td>160</td>\n",
       "      <td>46</td>\n",
       "      <td>61</td>\n",
       "      <td>256</td>\n",
       "      <td>0.795411</td>\n",
       "      <td>0.792135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB3_TFIDF</th>\n",
       "      <td>130</td>\n",
       "      <td>76</td>\n",
       "      <td>34</td>\n",
       "      <td>283</td>\n",
       "      <td>0.789675</td>\n",
       "      <td>0.761906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM_FINAL</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>58</td>\n",
       "      <td>259</td>\n",
       "      <td>0.789675</td>\n",
       "      <td>0.782304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_FINAL</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>60</td>\n",
       "      <td>257</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.779149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN3_TFIDF</th>\n",
       "      <td>119</td>\n",
       "      <td>87</td>\n",
       "      <td>25</td>\n",
       "      <td>292</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.749403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN_CVEC</th>\n",
       "      <td>153</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>258</td>\n",
       "      <td>0.785851</td>\n",
       "      <td>0.778299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN2_CVEC</th>\n",
       "      <td>157</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>252</td>\n",
       "      <td>0.782027</td>\n",
       "      <td>0.778544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR3_TFIDF</th>\n",
       "      <td>132</td>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>276</td>\n",
       "      <td>0.780115</td>\n",
       "      <td>0.755720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNB2_CVEC</th>\n",
       "      <td>159</td>\n",
       "      <td>47</td>\n",
       "      <td>69</td>\n",
       "      <td>248</td>\n",
       "      <td>0.778203</td>\n",
       "      <td>0.777090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR2_CVEC</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>247</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.763376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_CVEC</th>\n",
       "      <td>154</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>247</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.763376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_TFIDF</th>\n",
       "      <td>140</td>\n",
       "      <td>66</td>\n",
       "      <td>93</td>\n",
       "      <td>224</td>\n",
       "      <td>0.695985</td>\n",
       "      <td>0.693118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TN  FP  FN   TP       ACC       AUC\n",
       "MNB_FINAL   153  53  47  270  0.808795  0.797227\n",
       "MNB_TFIDF   131  75  25  292  0.808795  0.778529\n",
       "BN_TFIDF    123  83  22  295  0.799235  0.763843\n",
       "LR_TFIDF    138  68  38  279  0.797323  0.775015\n",
       "MNB_CVEC    160  46  61  256  0.795411  0.792135\n",
       "MNB3_TFIDF  130  76  34  283  0.789675  0.761906\n",
       "LSTM_FINAL  154  52  58  259  0.789675  0.782304\n",
       "BN_FINAL    154  52  60  257  0.785851  0.779149\n",
       "BN3_TFIDF   119  87  25  292  0.785851  0.749403\n",
       "BN_CVEC     153  53  59  258  0.785851  0.778299\n",
       "BN2_CVEC    157  49  65  252  0.782027  0.778544\n",
       "LR3_TFIDF   132  74  41  276  0.780115  0.755720\n",
       "MNB2_CVEC   159  47  69  248  0.778203  0.777090\n",
       "LR2_CVEC    154  52  70  247  0.766730  0.763376\n",
       "LR_CVEC     154  52  70  247  0.766730  0.763376\n",
       "XGB_TFIDF   140  66  93  224  0.695985  0.693118"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scorers.sort_values('ACC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Limitations\n",
    "\n",
    "It's evident that trying to discern whether a post was made in jest or made as a serious question is no easy task. Even though our highest model had an accuracy of 0.80, it still generated a significant number of false positives, and our AUC lags behind the accuracy as a result.\n",
    "\n",
    "Given this, however, it's still interesting that the model still managed to do as well as it did, considering that even as a human reading the post titles (that would be me here), I couldn't tell at times which subreddit a post belongs without checking for any reference elsewhere. \n",
    "\n",
    "Which brings me to the part on limitations. It might have been easier to get a better picture and a higher predictive score if we could also add in data within each post itself, for example, clicking into some of the posts from AskScience leads to long walls of text which actually discuss the core science question at hand.\n",
    "\n",
    "Within ShittyAskScience, it tends to be memes, funny images, or just short replies, since there was never really anything to discuss of in the first place. The key part in all of the limitations would be context - something even us humans need in order to be able to classify things the analog way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "Adding in comments within each post is definitely beneficial, as it provides a closer link to each post and hence allowing the machine to be able to learn and identify better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
